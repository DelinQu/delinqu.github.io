<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE">
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
    @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);

    /* Color scheme stolen from Sergey Karayev */
    a {
      /*color: #b60a1c;*/
      color: #1772d0;
      /*color: #bd0a36;*/
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Roboto', sans-serif;
      font-size: 15px;
      font-weight: 300;
    }

    strong {
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      /*font-family: 'Avenir Next';*/
      font-size: 15px;
      font-weight: 400;
    }

    heading {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 24px;
      font-weight: 400;
    }

    papertitle {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 15px;
      font-weight: 500;
    }

    name {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      font-weight: 400;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 140px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="media/website_icon.png">
  <title>Delin Qu - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
  <script src="script/functions.js"></script>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="80%" valign="middle">
              <p align="center">
                <name>Delin Qu 0010 屈德林</name>
              <div id="profile-email" align="center">dlqu22 at m dot fudan dot edu dot cn</div>
              </p>
              <p>
                I am a third-year Ph.D. candidate at the School of Computer Science, <a
                  href="https://www.fudan.edu.cn/en/">Fudan University</a> (FDU) and <a
                  href="http://www.shlab.org.cn/">Shanghai AI Laboratory</a>, Shanghai, China. I'm foutunate to be
                advised by <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ">Prof. Xuelong
                  Li</a> and be part of the <a href="https://huggingface.co/IPEC-COMMUNITY">IPEC@Team</a>. My research
                focuses on Embodied AI and 3D Computer Vision, with a long-term vision of achieving L2-level Physical
                Intelligence. I'm excited about the prospect of an "GPT moment" in Embodied AI, where AI systems can
                learn to interact with the physical world in a more human-like way.
              </p>
              <p>
                I was a research intern at <a href="http://www.shlab.org.cn/">Shanghai AI Laboratory</a> with <a
                  href="https://scholar.google.com/citations?user=ahUibskAAAAJ">Prof. Xuelong Li</a>. In Dec
                2024, I secured the National Natural Science Foundation of China (NSFC) grant to support my research.
              </p>
              <span>I will finish my PhD in fall 2027, and I am actively looking for research internship or exciting
                startup opportunities.</span>
              <p align=center>
                <a href="mailto:dlqu22@m.fudan.edu.cn">Email</a> &nbsp|&nbsp
                <a href="files/qudelin-en.pdf">CV</a> &nbsp|&nbsp
                <a href="https://github.com/DelinQu">GitHub</a> &nbsp|&nbsp
                <a href="https://scholar.google.com/citations?user=zgiFoOwAAAAJ">Google Scholar</a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/%E5%BE%B7%E6%9E%97-%E5%B1%88-8b9975280"> LinkedIn</a>
                &nbsp|&nbsp
                <a href="https://x.com/delin_qu23107">Twitter</a>
              </p>
            </td>
            <td width="20%">
              <img src="media/profile.png" width="200" alt="headshot">
            </td>
          </tr>
        </table>

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="10%" valign="middle">
              <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.is.mpg.de/"><img src="media/mpi_logo.png" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://research.google/"><img src="media/google_logo.png" width="55"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://about.facebook.com/realitylabs/"><img src="media/frl_logo.png" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="90"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="35"></a>
            </td>
          </tr>
        </table> -->

        <!-- News  -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>News</heading>
              <ul>
                <li><strong>Mar 2025</strong>: I am awarded the <a href="">Top Outstanding
                    Ph.D. Student Scholarship of Fudan University</a> (top 1%).</li>
                <li><b>Feb 2025</b> Our Lifelong Robot paper <a href="">Think Small, Act Big</a> is accepted at CVPR
                  2025. </li>
                <li><b>Mar 2025</b> I will also be a research interning at <a href="https://agibot.com/">AgiBot</a> this
                  summer on Foundation Vision-language-action models. </li>
                <li><b>Nov 2024</b> I have secured the <strong style='color:#c20000;'>National Natural Science
                    Foundation of
                    China (NSFC)</strong> grant to support my research. Deeply grateful to my <a
                    href="https://scholar.google.com/citations?user=ahUibskAAAAJ">supervisor</a> and <a
                    href="https://github.com/SHAILAB-IPEC">research team@IPEC</a> </li>
                <li><b>Sep 2024</b> Our paper <a href="https://livescenes.github.io/">LiveScene
                    Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control</a> is
                  accepted at Neurips 2024. </li>
                <li><strong>May 2024</strong>: We integrated KAN into NeRF and conducted a preliminary evaluation, <a
                    href="https://t.co/5cus4thPrT">integrated KAN into NeRF 😆</a>, <a
                    href="https://github.com/Tavish9/KANeRF">Hands-On NeRF with KAN</a>!</li>
                <li><strong>Feb 2024</strong><img src="media/logo_oral.jpg" width="20"><a
                    href="https://gs-slam.github.io/">Our papers <strong>GS-SLAM</strong></a> and <a
                    href="https://delinqu.github.io/EN-SLAM/"><strong>EN-SLAM</strong></a> are both accepted to
                  <strong>CVPR
                    2024</strong> as <strong><span style="color:#c20000;">Highlight Paper</span> (top 2.6% ×
                    2)</strong>.
                </li>
                <li><strong>Oct 2023</strong>: I am awarded the <a href="https://www.tencent.com/en-us/">Tencent
                    Scholarship</a> (top 0.1%).</li>
                <li><strong>Jun 2023</strong>: Our paper <a
                    href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qu_Towards_Nonlinear-Motion-Aware_and_Occlusion-Robust_Rolling_Shutter_Correction_ICCV_2023_paper.pdf">Towards
                    Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction</a> got accepted to ICCV
                  2023</li>
                <li><strong>Jun 2023</strong>:<img src="media/logo_oral.jpg" width="20">The extension of my undergraduate thesis <a
                    href="https://ieeexplore.ieee.org/document/10148802">Fast Rolling Shutter Correction in the Wild</a>
                  got accepted to <span style="color:#c20000;"><strong>TPAMI</strong></span>!</li>
                <li><strong>Mar 2023</strong>: Our paper <a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Revisiting_Rolling_Shutter_Bundle_Adjustment_Toward_Accurate_and_Fast_Solution_CVPR_2023_paper.pdf">Revisiting
                    Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution</a> got accepted to CVPR
                  2023.</li>
                <li><strong>Sep 2018</strong>: Start my PhD journey at <a href="https://www.fudan.edu.cn/en">Fudan
                    University</a>!</li>

                <!-- 🔥 NOTE: below is old news  -->
                <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>

                <div id="old_news" style="display: none;">
                  <li><strong>Aug 2018</strong>: I start my research internship at <a
                      href="http://www.shlab.org.cn/">Shanghai AI Laboratory @ EPIC</a> with <a
                      href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Prof. Bin Zhao</a>!
                  </li>
                  <li><strong>Jul 2018</strong>: Graduated from <a href="https://www-en.hnu.edu.cn">Hunan University</a>
                    in computer science and technology, Bachelor, Ranking 1/208, with National Scholarship and
                    Outstanding Graduate.
                  </li>
                </div>
              </ul>
            </td>
          </tr>
        </table>
        <!-- Research  -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Research</heading>
          <tr onmouseout="spatial_stop()" onmouseover="spatial_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='spatial_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202502-spatialvla/spatialvla.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202502-spatialvla/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function spatial_start() {
                  document.getElementById('spatial_shape').style.opacity = "1";
                }
                function spatial_stop() {
                  document.getElementById('spatial_shape').style.opacity = "0";
                }
                spatial_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://spatialvla.github.io">
                <papertitle>
                  SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://github.com/HaomingSong">Haoming Song*</a>,
              <a href="https://github.com/Tavish9">Qizhi Chen*</a>,
              <a href="https://ceciliayao.github.io/">Yuanqi Yao</a>,
              <a href="https://scholar.google.com/citations?user=GlYeyfoAAAAJ">Xinyi Ye</a>,
              <a href="https://cseweb.ucsd.edu/~jigu/">Jiayuan Gu</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <br>
              <!-- <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025 <strong>(<span style="color:#c20000;">Oral</span>, top 1.8%)</strong>
              <br> -->
              <a href="https://arxiv.org/abs/2501.15830">paper</a> |
              <a href="https://spatialvla.github.io">project page</a> |
              <a href="https://spatialvla.github.io">video</a> |
              <a href="https://github.com/SpatialVLA/SpatialVLA">code</a> |
              <a
                href="https://huggingface.co/collections/IPEC-COMMUNITY/foundation-vision-language-action-model-6795eb96a9c661f90236acbb">model
                <img alt="Static Badge" src="https://img.shields.io/badge/🤗 Huggingface-spatialvla-orange">
              </a>
              <br>
              A spatial-enhanced vision-language-action model trained on 1.1 Million real robot episodes, purely
              huggingFace-based, concise code with efficient performance.
            </td>
          </tr>

          <tr onmouseout="freegs_stop()" onmouseover="freegs_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='freegs_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202502-freegaussian/freegaussian.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202502-freegaussian/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function freegs_start() {
                  document.getElementById('freegs_shape').style.opacity = "1";
                }
                function freegs_stop() {
                  document.getElementById('freegs_shape').style.opacity = "0";
                }
                freegs_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://freegaussian.github.io">
                <papertitle>
                  FreeGaussian: Guidance-free Controllable 3D Gaussian Splats with Flow Derivatives
                </papertitle>
              </a>
              <br>
              <a href="https://github.com/Tavish9">Qizhi Chen*</a>,
              <strong>Delin Qu*</strong>,
              <a href="https://github.com/HaomingSong">Haoming Song</a>,
              <a href="https://scholar.google.com.hk/citations?user=v-oVANQAAAAJ">Yiwen Tang</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <br>
              <!-- <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025 <strong>(<span style="color:#c20000;">Oral</span>, top 1.8%)</strong>
              <br> -->
              <a href="https://arxiv.org/abs/2410.22070">paper</a> |
              <a href="https://freegaussian.github.io">project page</a> |
              <a href="https://freegaussian.github.io">video</a> |
              <a href="https://github.com/freegaussian/freegaussian.github.io">code</a>
              <br>
              An annotation guidance-free method, dubbed FreeGaussian, that mathematically derives dynamic Gaussian
              motion from optical flow and camera motion using novel dynamic Gaussian constraints.
            </td>
          </tr>

          <tr onmouseout="livescene_stop()" onmouseover="livescene_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='livescene_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202409-livescene/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202409-livescene/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function livescene_start() {
                  document.getElementById('livescene_shape').style.opacity = "1";
                }
                function livescene_stop() {
                  document.getElementById('livescene_shape').style.opacity = "0";
                }
                livescene_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://livescenes.github.io/">
                <papertitle>
                  LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://github.com/Tavish9">Qizhi Chen*</a>,
              <a href="https://github.com/zhangpingrui">Pingrui Zhang</a>,
              <a href="https://scholar.google.com/citations?user=oZSREOkAAAAJ">Xianqiang Gao</a>,
              <!-- <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>, -->
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <!-- <a href="https://scholar.google.com/citations?user=cw3EaAYAAAAJ">Zhigang Wang</a>, -->
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>Neurips</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2406.16038">paper</a> |
              <a href="https://livescenes.github.io">project page</a> |
              <a href="https://livescenes.github.io">video</a> |
              <a href="https://github.com/Tavish9/livescene">code</a> |
              <a href="https://huggingface.co/datasets/IPEC-COMMUNITY/LiveScene">dataset <img alt="Static Badge"
                  src="https://img.shields.io/badge/🤗 Huggingface-livescene-green"></a>
              <br>
              Embedding language feature to interactive scenes, grounding and manipulating interactable objects with
              language instructions.
            </td>
          </tr>

          <tr onmouseout="gsslam_stop()" onmouseover="gsslam_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='gsslam_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202403-gsslam/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202403-gsslam/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function gsslam_start() {
                  document.getElementById('gsslam_shape').style.opacity = "1";
                }
                function gsslam_stop() {
                  document.getElementById('gsslam_shape').style.opacity = "0";
                }
                gsslam_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://gsslams.github.io/">
                <papertitle>
                  <img src="media/logo_oral.jpg" width="15">GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting
                </papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=IuGoHCkAAAAJ">Chi Yan*</a>,
              <strong>Delin Qu*</strong>,
              <a href="https://www.danxurgb.net">Dan Xu</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?user=cw3EaAYAAAAJ">Zhigang Wang</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024, <strong>(<span style="color:#c20000;">Spotlight</span>, top 2.6%)</strong>
              <br>
              <a href="https://arxiv.org/abs/2311.11700">paper</a> |
              <a href="https://gs-slam.github.io/">project page</a> |
              <a href="https://gs-slam.github.io/">video</a> |
              <a href="https://github.com/yanchi-3dv/diff-gaussian-rasterization-for-gsslam">code</a>
              <br>
              The first to utilize 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system.
            </td>
          </tr>

          <tr onmouseout="enslam_stop()" onmouseover="enslam_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='enslam_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202403-enslam/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202403-enslam/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function enslam_start() {
                  document.getElementById('enslam_shape').style.opacity = "1";
                }
                function enslam_stop() {
                  document.getElementById('enslam_shape').style.opacity = "0";
                }
                enslam_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://delinqu.github.io/EN-SLAM/">
                <papertitle>
                <img src="media/logo_oral.jpg" width="15">Implicit Event-RGBD Neural SLAM
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://scholar.google.com/citations?user=IuGoHCkAAAAJ">Chi Yan*</a>,
              <a href="https://scholar.google.com/citations?user=Y8LVRYIAAAAJ&hl=en">Yin Jie</a>,
              <a href="https://github.com/Tavish9">Qizhi Chen</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?user=cw3EaAYAAAAJ">Zhigang Wang</a>,
              <a href="https://www.danxurgb.net">Dan Xu</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024, <strong>(<span style="color:#c20000;">Spotlight</span>, top 2.6%)</strong>
              <br>
              <a href="https://arxiv.org/abs/2311.11013">paper</a> |
              <a href="https://delinqu.github.io/EN-SLAM">project page</a> |
              <a href="https://delinqu.github.io/EN-SLAM">video</a> |
              <a href="https://delinqu.github.io/EN-SLAM">code</a> |
              <a href="https://huggingface.co/datasets/delinqu/EN-SLAM-Dataset">dataset <img alt="Static Badge" src="https://img.shields.io/badge/🤗 Huggingface-enslam-green"></a>
              <br>
              The first event-RGBD implicit neural SLAM that leverages event stream and RGBD to overcome challenges in motion blur and lighting variation scenes.
            </td>
          </tr>

          <tr onmouseout="qrsc_stop()" onmouseover="qrsc_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='qrsc_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202406-qrst/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202406-qrst/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function qrsc_start() {
                  document.getElementById('qrsc_shape').style.opacity = "1";
                }
                function qrsc_stop() {
                  document.getElementById('qrsc_shape').style.opacity = "0";
                }
                qrsc_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://delinqu.github.io/QRSC">
                <papertitle>
                Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://yizhenlao.github.io">Yizhen Lao</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.com/citations?user=cw3EaAYAAAAJ">Zhigang Wang</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <br>
              <em>Proceedings of the IEEE/CVF International Conference on Computer Vision(<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qu_Towards_Nonlinear-Motion-Aware_and_Occlusion-Robust_Rolling_Shutter_Correction_ICCV_2023_paper.pdf">paper</a> |
              <a href="https://delinqu.github.io/QRSC/">project page</a> |
              <a href="https://www.youtube.com/watch?v=Or-yvKHUrZ0">video</a> |
              <a href="https://github.com/DelinQu/qrsc?tab=readme-ov-file">code</a>
              <br>
              A geometry-based Quadratic Rolling Shutter (QRS) motion solver, which precisely estimates the high-order correction field of individual pixels.
            </td>
          </tr>

          <tr onmouseout="rsba_stop()" onmouseover="rsba_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='rsba_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202303-rsba/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202303-rsba/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function rsba_start() {
                  document.getElementById('rsba_shape').style.opacity = "1";
                }
                function rsba_stop() {
                  document.getElementById('rsba_shape').style.opacity = "0";
                }
                rsba_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://delinqu.github.io/NW-RSBA">
                <papertitle>
                  Revisiting Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution
                </papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=0z2qluIAAAAJ">Bangyan Liao*</a>,
              <strong>Delin Qu*</strong>,
              <a href="https://dblp.org/pid/58/6739.html">Yifei Xue</a>,
              <a href="https://github.com/Kikihqq">Huiqing Zhang</a>,
              <a href="https://yizhenlao.github.io">Yizhen Lao</a>.
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Revisiting_Rolling_Shutter_Bundle_Adjustment_Toward_Accurate_and_Fast_Solution_CVPR_2023_paper.pdf">paper</a> |
              <a href="https://delinqu.github.io/NW-RSBA">project page</a> |
              <a href="https://www.youtube.com/watch?v=aCo60XUatss">video</a> |
              <a href="https://github.com/DelinQu/NW-RSBA">code</a>
              <br>
              An accurate and fast bundle adjustment solution that estimates the 6-DoF pose with an independent RS model of the camera and the geometry of the environment based on measurements from a rolling shutter camera.
            </td>
          </tr>

          <tr onmouseout="drsc_stop()" onmouseover="drsc_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='drsc_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202306-drsc/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202306-drsc/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function drsc_start() {
                  document.getElementById('drsc_shape').style.opacity = "1";
                }
                function drsc_stop() {
                  document.getElementById('drsc_shape').style.opacity = "0";
                }
                drsc_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://github.com/DelinQu/rspy">
                <papertitle>
                  <img src="media/logo_oral.jpg" width="15">
                  Fast Rolling Shutter Correction in the Wild
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://scholar.google.com/citations?user=0z2qluIAAAAJ">Bangyan Liao*</a>,
              <a href="https://dblp.org/pid/58/6739.html">Yifei Xue</a>,
              <a href="https://github.com/Kikihqq">Huiqing Zhang</a>,
              <a href="https://scholar.google.fr/citations?user=NIdLQnUAAAAJ&hl=fr">Omar Ait Aider</a>,
              <a href="https://yizhenlao.github.io">Yizhen Lao</a>.
              <br>
              IEEE Transactions on Pattern Analysis and Machine Intelligence <strong>(<span style="color:#c20000;"><em>TPAMI</em></span>)</strong>, 2023
              <br>
              <a href="https://ieeexplore.ieee.org/document/10148802">paper</a> |
              <a href="https://github.com/DelinQu/rspy">project page</a> |
              <a href="https://github.com/DelinQu/rspy">video</a> |
              <a href="https://github.com/DelinQu/rspy">code</a> |
              <a href="https://github.com/DelinQu/Urban-Crossroads-Rolling-Shutter-Dataset">dataset <img alt="Static Badge" src="https://img.shields.io/badge/🤗 Huggingface-fast-green"></a>
              <br>
              A pixel-wise varying direct RS correction framework that handles locally varying distortion caused by various sources, such as camera motion, moving objects, and even highly varying depth scenes.
            </td>
          </tr>
        </table>

        <!-- Mentored Students -->
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Mentored Students</heading>
          <tr>
            <td>
              I am fortunate to (co-)mentor some talented and highly motivated
              students. I
              have learnt from and gotten inspired by them:
              <ul>
                <li> <a href="https://janackermann.info/"><strong>Jan
                      Ackermann</strong></a> (Ongoing): MSc student at ETH
                  Zurich<br>
                  <ul>
                    <li> Semester thesis: Continual Learning of Gaussian Splatting
                      with
                      Local Optimization (CVPR'25 submission)
                    <li> &#8594; Master thesis at Stanford University, advised by <a
                        href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
                  </ul>
                  <br>
                <li> <a href="https://www.linkedin.com/in/goncayilmaz/"><strong>Gonca
                      Yilmaz</strong></a> (2024): MSc student at University of
                  Zurich<br>
                  <ul>
                    <li> Semester thesis: Open Vocabulary Segmentation from
                      Multi-Modal
                      Inputs (ICCVW'23)
                    <li> Master thesis: <a href="https://open-das.github.io/">OpenDAS:
                        Open-Vocabulary Domain Adaption for Segmentation</a>
                      (ICLR'25
                      submission)
                    <li> &#8594; Software engineer at <a href="https://about.google/">Google</a>
                  </ul>
                  <br>
            </td>
          </tr>
        </table> -->


        <!-- Invited Talks -->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;"> -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Invited Talks</heading>
          <br><br><br>
          <tr onmouseout="teleai_stop()" onmouseover="teleai_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='talk_teleai'>
                  <img src='talk/202503_teleai/teaser2.png' width="180" height="100">
                </div>
                <img src='talk/202503_teleai/teaser1.png' width="180" height="100">
              </div>
              </div>
              <script type="text/javascript">
                function teleai_start() {
                  document.getElementById('talk_teleai').style.opacity = "1";
                }
                function teleai_stop() {
                  document.getElementById('talk_teleai').style.opacity = "0";
                }
                teleai_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="talk/202503_teleai/spatialvla_slides.pdf">
                <papertitle>Exploring Spatial Representations for Visual-Language-Action Model</papertitle>
              </a>
              <br>
              <em><strong>Institute of Artificial Intelligence (TeleAI), China Telecom</strong></em>, hosted by <a
                href="https://baichenjia.github.io/">Chenjia Bai</a>, Mar 2025<br>
              <br>
              A spatial-enhanced vision-language-action model that is trained on 1.1 Million real robot episodes,
              toward the More Generalist Agents System.
              <a href="talk/202503_teleai/spatialvla_slides.pdf">slides</a>
            </td>
          </tr>
        </table>


        <!-- Selected Projects -->
        <table width="100%" align="center" border="0" cellpadding="20" cellspacing="0">
          <heading>Selected Projects</heading>
          <br><br><br>
          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='fastumi'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="project/fast-umi/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://fastumi.com">
                <papertitle>
                  <img src="media/logo_oral.jpg" width="15"> FastUMI: A Scalable and Hardware-Independent Universal
                  Manipulation Interface with Dataset
                </papertitle>
              </a>
              <br>
              Zhaxizhuoma, Kehui Liu, et.al, <strong>Delin Qu</strong>, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li
              <!-- <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025 <strong>(<spanstyle="color:#c20000;">Oral</span>, top 1.8%)</strong>
              <br> -->
              <a href="https://arxiv.org/abs/2409.19499">paper</a> |
              <a href="https://fastumi.com/">project page</a> |
              <a href="https://fastumi.com/">video</a> |
              <a href="https://github.com/RealRobotSquad/FastUMI_Data">code</a>
              <p></p>
              A substantial redesign of the Universal Manipulation Interface system enabling rapid deployment and
              delivering robust performance in real-world data acquisition.
              <p></p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='embllm'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="project/ood/ood.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="http://scis.scichina.com/en/2024/124201.pdf">
                <papertitle>
                  Optics-driven drone
                </papertitle>
              </a>
              <br>
              Xuelong Li, Guan Huang, Zhigang Wang, <strong>Delin Qu</strong>, Bin Zhao
              <br>
              <em>Science China. Information Sciences, 67(2), 124201</em>, 2024
              <br>
              <a href="http://scis.scichina.com/en/2024/124201.pdf">paper</a> |
              <a href="http://scis.scichina.com/en/2024/124201.pdf">project page</a> |
              <a href="http://scis.scichina.com/en/2024/124201.pdf">video</a> |
              <a href="http://scis.scichina.com/en/2024/124201.pdf">code</a>
              <p></p>
              A remote charging technology for drones to enhance their autonomy and intelligence during mission
              execution
              <p></p>
            </td>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='embllm'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="project/embllm/embllm.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="http://scis.scichina.com/en/2024/124201.pdf">
                <papertitle>
                  Large Model Heterogeneous Intelligent Agent Systems
                </papertitle>
              </a>
              <br>
              Kehui Liu, Zixin Tang, et.al, <strong>Delin Qu</strong>, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li
              <br>
              <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2409.15146v2">paper</a> |
              <a href="https://github.com/mrkeee/coherent">project page</a> |
              <a href="https://www.youtube.com/watch?v=dV1J-VXdEJA">video</a> |
              <a href="https://github.com/mrkeee/coherent">code</a>
              <p></p>
              A novel LLM-based task planning framework for collaboration of heterogeneous multi-robot systems including
              quadrotors, robotic dogs, and robotic arms.
              <p></p>
            </td>
          </tr>
    </tr>

  </table>

  <!-- Honors & Awards -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>Honors & Awards</heading>
    <tr>
      <td>
        <ul>
          <li><strong>Sep 2022 - Now</strong>: Top Outstanding PhD Student Scholarship of Fudan University in
            2025, Tencent Scholarship in 2023, Fudan University Master's Excellence Scholarship in 2022,
            Outstanding Student Award in 2023, Fudan University's Outstanding Youth League Member in 2024.</li>
          <li><strong>Sep 2018 - Jun 2022</strong>: National Scholarship in 2021, National Scholarship in 2020,
            National Inspirational Scholarship in 2019, Finalist Prize of Mathematical Contest in Modeling, Second
            Prize of Asia-Pacific Mathematical Contest in Modeling, Second Prize in National Internet of Things
            Design Contest, Second Prize in Internet Competition of Hunan Province, Excellence Award in the Huawei
            AI Cloud Cup, Huawei College Scholarship, Huawei Smart Base Future Star, Excellent Graduation Thesis.
          </li>
        </ul>
      </td>
    </tr>
  </table>

  <!-- Academic Services -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>Academic Services</heading>
    <tr>
      <td>
        <ul>
          <li><strong>Conference Reviewer</strong>: CVPR, ICCV, ECCV, ICLR, ICML, and NeurIPS.<br></li>
          <li><strong>2023 Spring</li>strong>: COMP130135.04 Object Oriented Programming, Teaching Assistant.
          </li>
        </ul>
      </td>
    </tr>
  </table>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <br>
        <p align="right">
          <font size="2">
            template adapted from <a href="https://jonbarron.info/">
              <font size="2">this awesome website</font>
            </a>
          </font>
        </p>
      </td>
    </tr>
  </table>
  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script>
  <script type="text/javascript">
    try {
      var pageTracker = _gat._getTracker("UA-116734954-1");
      pageTracker._trackPageview();
    } catch (err) { }
  </script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-116734954-1');
  </script>
  </td>
  </tr>
  </table>
</body>

</html>