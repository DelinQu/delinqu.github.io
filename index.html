<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE">
  <meta name=viewport content=‚Äúwidth=800‚Äù>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
    @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);

    /* Color scheme stolen from Sergey Karayev */
    a {
      /*color: #b60a1c;*/
      color: #1772d0;
      /*color: #bd0a36;*/
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Roboto', sans-serif;
      font-size: 15px;
      font-weight: 300;
    }

    strong {
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      /*font-family: 'Avenir Next';*/
      font-size: 15px;
      font-weight: 400;
    }

    heading {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 24px;
      font-weight: 400;
    }

    papertitle {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 15px;
      font-weight: 500;
    }

    name {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      font-weight: 400;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 140px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="media/website_icon.png">
  <title>Delin Qu - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
  <script src="script/functions.js"></script>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="80%" valign="middle">
              <p align="center">
                <name>Delin Qu 0010 Â±àÂæ∑Êûó</name>
              <div id="profile-email" align="center">dlqu22 at m dot fudan dot edu dot cn</div>
              </p>
              <p>
                I am a fourth-year Ph.D. candidate at the School of Computer Science, <a
                  href="https://www.fudan.edu.cn/en/">Fudan University</a> (FDU) and <a
                  href="http://www.shlab.org.cn/">Shanghai AI Laboratory</a>, Shanghai, China. I'm foutunate to be
                advised by <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ">Prof. Xuelong
                  Li</a> and be part of the <a href="http://eo-robotics.ai/">EO-Robotics Teamn</a>. My research
                focuses on Embodied AI and 3D Computer Vision, with a long-term vision of achieving L2-level Physical
                Intelligence. I'm excited about the prospect of an "GPT moment" in Embodied AI, where AI systems can
                learn to interact with the physical world in a more human-like way.
              </p>
              <p>
                I was a research intern at <a href="http://www.shlab.org.cn/">Shanghai AI Laboratory</a> with <a
                  href="https://scholar.google.com/citations?user=ahUibskAAAAJ">Prof. Xuelong Li</a>. In Dec
                2024, I secured the National Natural Science Foundation of China (NSFC) grant to support my research.
              </p>
              <span>I will finish my PhD in fall 2027, and I am actively looking for research internship or exciting
                startup opportunities.</span>
              <p align=center>
                <a href="mailto:dlqu22@m.fudan.edu.cn">Email</a> &nbsp|&nbsp
                <!-- <a href="files/qudelin-en.pdf">CV</a> &nbsp|&nbsp -->
                <a href="https://github.com/DelinQu">GitHub</a> &nbsp|&nbsp
                <a href="https://scholar.google.com/citations?user=zgiFoOwAAAAJ">Google Scholar</a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/%E5%BE%B7%E6%9E%97-%E5%B1%88-8b9975280"> LinkedIn</a> &nbsp|&nbsp
                <a href="https://x.com/delin_qu23107">Twitter</a> &nbsp|&nbsp
                <a href="https://www.xiaohongshu.com/user/profile/61f22e8f0000000010007002">Xhs</a>
              </p>
            </td>
            <td width="20%">
              <img src="media/profile.png" width="200" alt="headshot">
            </td>
          </tr>
        </table>

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="10%" valign="middle">
              <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.is.mpg.de/"><img src="media/mpi_logo.png" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://research.google/"><img src="media/google_logo.png" width="55"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://about.facebook.com/realitylabs/"><img src="media/frl_logo.png" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="90"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="35"></a>
            </td>
          </tr>
        </table> -->

        <div class="container">
          <img src="media/generalist_robot_policies.png" alt="generalist_robot_policies" style="width: 45%;">
          <img src="media/research_problem.png" alt="research_problem" style="width: 49.5%;">
        </div>

        <!-- News  -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>News</heading>
              <ul>
                <li><strong>Dec 2025</strong> üèÜ Our Team <strong>Comet</strong> wins 2nd place in the
                  <a href="https://behavior.stanford.edu/challenge/index.html">BEHAVIOR Challenge</a> led by Dr Fei-Fei Li and the Stanford University team. üëè
                  Check our solution here! <a href="https://github.com/mli0603/openpi-comet">Openpi-Comet</a>.
                </li>
                <li><strong>Dec 2025</strong><img src="media/logo_oral.jpg" width="20"><a
                  href="https://github.com/Tavish9/freegaussian">Our paper <strong>FreeGaussian</strong></a> is accepted to
                  <strong><span style="color:#c20000;">AAAI 2025 Oral</span></strong>.
                </li>
                <li><strong>Sep 2025</strong>
                  We introduce <img src="media/eo_logo_bare.png" width="15"> <a
                  href="http://eo-robotics.ai"><strong>EO-1</strong></a>
                  , an open-source unified embodied foundation model trained on Interleaved Embodied Multimodal Data, integrates discrete auto-regressive decoding with continuous flow matching for multimodal embodied reasoning and robot control.
                  <strong>EO-Robotic Team: <a href="http://eo-robotics.ai">http://eo-robotics.ai</a></strong>.
                </li>
                <li><strong>Apr 2025</strong><img src="media/logo_oral.jpg" width="20"><a
                    href="https://spatialvla.github.io/">Our papers <strong>SpatialVLA</strong></a> are accepted to
                  <strong>Robotics: Science and Systems 2025</strong>. I‚Äôll be presenting a short <strong><span style="color:#c20000;">Spotlight Talk</span></strong> in RSS, LA. Meanwhile, I‚Äôm actively exploring <strong>internships or visiting opportunities</strong>. Open to exciting collaborations‚ÄîDM or email me!
                </li>
                <li><strong>Mar 2025</strong>: I am awarded the <a href="">Top Outstanding
                    Ph.D. Student Scholarship of Fudan University</a> (top 1%).</li>
                <li><strong>Feb 2025</strong> Our Lifelong Robot paper <a href="">Think Small, Act Big</a> is accepted at CVPR
                  2025. </li>
                <li><strong>Mar 2025</strong> I will also be a research interning at <a href="https://agibot.com/">AgiBot</a> this
                  summer on Foundation Vision-language-action models. </li>
                <li><strong>Nov 2024</strong> I have secured the <strong style='color:#c20000;'>National Natural Science
                    Foundation of
                    China (NSFC)</strong> grant to support my research. Deeply grateful to my <a
                    href="https://scholar.google.com/citations?user=ahUibskAAAAJ">supervisor</a> and <a
                    href="https://github.com/SHAILAB-IPEC">research team@IPEC</a> </li>
                <li><strong>Dec 2024</strong><img src="media/logo_oral.jpg" width="20"><a
                    href="https://goxq.github.io/mifag/">Our paper <strong>Learning 2D Invariant Affordance Knowledge for 3D Affordance Grounding</strong></a> is accepted to
                  <strong>AAAI 2025</strong> as <strong><span style="color:#c20000;">Oral Paper</span> (top 4.6%)</strong>.
                </li>
                <li><strong>Sep 2024</strong> Our paper <a href="https://livescenes.github.io/">LiveScene
                    Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control</a> is
                  accepted at Neurips 2024. </li>
                <li><strong>May 2024</strong>: We integrated KAN into NeRF and conducted a preliminary evaluation, <a
                    href="https://t.co/5cus4thPrT">integrated KAN into NeRF üòÜ</a>, <a
                    href="https://github.com/Tavish9/KANeRF">Hands-On NeRF with KAN</a>!</li>
                <li><strong>Feb 2024</strong><img src="media/logo_oral.jpg" width="20"><a
                    href="https://gs-slam.github.io/">Our papers <strong>GS-SLAM</strong></a> and <a
                    href="https://delinqu.github.io/EN-SLAM/"><strong>EN-SLAM</strong></a> are both accepted to
                  <strong>CVPR
                    2024</strong> as <strong><span style="color:#c20000;">Two Highlight Paper</span> (top 2.6% √ó
                    2)</strong>.
                </li>
                <li><strong>Oct 2023</strong>: I am awarded the <a href="https://www.tencent.com/en-us/">Tencent
                    Scholarship</a> (top 0.1%).</li>
                <li><strong>Jun 2023</strong>: Our paper <a
                    href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qu_Towards_Nonlinear-Motion-Aware_and_Occlusion-Robust_Rolling_Shutter_Correction_ICCV_2023_paper.pdf">Towards
                    Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction</a> got accepted to ICCV
                  2023</li>
                <li><strong>Jun 2023</strong>:<img src="media/logo_oral.jpg" width="20">The extension of my
                  undergraduate thesis <a href="https://ieeexplore.ieee.org/document/10148802">Fast Rolling Shutter
                    Correction in the Wild</a>
                  got accepted to <span style="color:#c20000;"><strong>TPAMI</strong></span>!</li>
                <li><strong>Mar 2023</strong>: Our paper <a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Revisiting_Rolling_Shutter_Bundle_Adjustment_Toward_Accurate_and_Fast_Solution_CVPR_2023_paper.pdf">Revisiting
                    Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution</a> got accepted to CVPR
                  2023.</li>
                <li><strong>Sep 2022</strong>: Start my PhD journey at <a href="https://www.fudan.edu.cn/en">Fudan
                    University</a>!</li>

                <!-- üî• NOTE: below is old news  -->
                <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>

                <div id="old_news" style="display: none;">
                  <li><strong>Aug 2022</strong>: I start my research internship at <a
                      href="http://www.shlab.org.cn/">Shanghai AI Laboratory @ EPIC</a> with <a
                      href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Prof. Bin Zhao</a>!
                  </li>
                  <li><strong>Jul 2022</strong>: Graduated from <a href="https://www-en.hnu.edu.cn">Hunan University</a>
                    in computer science and technology, Bachelor, Ranking 1/208, with National Scholarship and
                    Outstanding Graduate.
                  </li>
                </div>
              </ul>
            </td>
          </tr>
        </table>
        <!-- Research  -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Research</heading>
          <tr onmouseout="eo1_stop()" onmouseover="eo1_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='eo1_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202509-eo1/video.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202509-eo1/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function eo1_start() {
                  document.getElementById('eo1_shape').style.opacity = "1";
                }
                function eo1_stop() {
                  document.getElementById('eo1_shape').style.opacity = "0";
                }
                eo1_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://eo-robotics.ai/eo-1">
                <papertitle>EO-1: Interleaved Vision-Text-Action Pretraining for General Robot Control,</papertitle>
              </a>
              <strong>Delin Qu*</strong>,
              <a href="https://github.com/HaomingSong">Haoming Song*</a>,
              <a href="https://github.com/Tavish9">Qizhi Chen*</a>,
              <a href="">Zhaoqin Chen*</a>,
              <a href="https://scholar.google.com/citations?user=oZSREOkAAAAJ">Xianqiang Gao*</a>,
              <a href="https://scholar.google.com/citations?user=oqN1dA8AAAAJ">Guanghui Ren</a>,
              <a href="https://scholar.google.com/citations?user=N36nqu4AAAAJ">Maoqing Yao</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              et al. <strong><a href="http://eo-robotics.ai">EO-Robotic Team</a></strong>
              <br>
              <a href="http://eo-robotics.ai">
                <img
                  src="https://img.shields.io/badge/EO--Robotics-Website-5865F2?logo=googleplay&logoColor=white"
                  alt="EO-Robotics Website"
                />
              </a>
              <a href="https://arxiv.org/abs/2508.21112">
                <img
                  src="https://img.shields.io/badge/EO--1-Paper-red?logo=arxiv&logoColor=red"
                  alt="EO-Robotics Paper on arXiv"
                />
              </a>
              <a href="https://huggingface.co/IPEC-COMMUNITY/EO-1-3B">
                <img 
                    src="https://img.shields.io/badge/EO--1--3B-Model-FFCC11?logo=huggingface&logoColor=brightyellow" 
                    alt="EO-1 Model"
                />
              </a>
              <a href="https://huggingface.co/datasets/IPEC-COMMUNITY/EO-Data1.5M">
                <img
                  src="https://img.shields.io/badge/Dataset-EO--Data1.5M-brightgreen?logo=huggingface&logoColor=brightyellow"
                  alt="EO-1.5M"
                />
              </a>
              <br>
              An open-source unified embodied foundation model integrates discrete auto-regressive decoding with continuous flow matching for embodied reasoning and robot control.
            </td>
          </tr>

          <tr onmouseout="hume_stop()" onmouseover="hume_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='hume_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202506-hume/video.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202506-hume/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function hume_start() {
                  document.getElementById('hume_shape').style.opacity = "1";
                }
                function hume_stop() {
                  document.getElementById('hume_shape').style.opacity = "0";
                }
                hume_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://hume-vla.github.io">
                <papertitle>
                  Hume: Introducing System-2 Thinking in Visual-Language-Action Model
                </papertitle>
              </a>
              <br>
              <a href="https://github.com/HaomingSong">Haoming Song*</a>,
              <strong>Delin Qu*</strong>,
              <a href="https://ceciliayao.github.io/">Yuanqi Yao</a>,
              <a href="https://github.com/Tavish9">Qizhi Chen</a>,
              <a href="https://scholar.google.com/citations?user=GlYeyfoAAAAJ">Xinyi Ye</a>,
              <a href="https://github.com/aopolin-lv">Qi Lv</a>,
              <a href="https://scholar.google.com/citations?user=oZSREOkAAAAJ">Xianqiang Gao*</a>,
              <a href="https://scholar.google.com/citations?user=oqN1dA8AAAAJ">Guanghui Ren</a>,
              <a href="https://scholar.google.com/citations?user=N36nqu4AAAAJ">Maoqing Yao</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <br>
              <a href="https://arxiv.org/abs/2505.21432">paper</a> |
              <a href="https://hume-vla.github.io">project page</a> |
              <a href="https://hume-vla.github.io">video</a> |
              <a href="https://github.com/hume-vla/hume">code</a> |
              <a
                href="https://huggingface.co/collections/Hume-vla/hume-model-zoo-684be6e5d062717593589a9a">model
                <img alt="Static Badge" src="https://img.shields.io/badge/ü§ó Huggingface-Hume-orange">
              </a>
              <br>
              A spatial-enhanced vision-language-action model trained on 1.1 Million real robot episodes, purely
              huggingFace-based, concise code with efficient performance.
            </td>
          </tr>

          <tr onmouseout="spatial_stop()" onmouseover="spatial_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='spatial_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202502-spatialvla/spatialvla.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202502-spatialvla/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function spatial_start() {
                  document.getElementById('spatial_shape').style.opacity = "1";
                }
                function spatial_stop() {
                  document.getElementById('spatial_shape').style.opacity = "0";
                }
                spatial_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://spatialvla.github.io">
                <papertitle>
                  SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://github.com/HaomingSong">Haoming Song*</a>,
              <a href="https://github.com/Tavish9">Qizhi Chen*</a>,
              <a href="https://ceciliayao.github.io/">Yuanqi Yao</a>,
              <a href="https://scholar.google.com/citations?user=GlYeyfoAAAAJ">Xinyi Ye</a>,
              <a href="https://cseweb.ucsd.edu/~jigu/">Jiayuan Gu</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <br>
              <em>Robotics: Science and Systems (<strong>RSS</strong>)</em>, 2025 <strong>(<span style="color:#c20000;">Spotlight</span>)</strong>
              <br>
              <a href="https://arxiv.org/abs/2501.15830">paper</a> |
              <a href="https://spatialvla.github.io">project page</a> |
              <a href="https://spatialvla.github.io">video</a> |
              <a href="https://github.com/SpatialVLA/SpatialVLA">code</a> |
              <a
                href="https://huggingface.co/collections/IPEC-COMMUNITY/foundation-vision-language-action-model-6795eb96a9c661f90236acbb">model
                <img alt="Static Badge" src="https://img.shields.io/badge/ü§ó Huggingface-spatialvla-orange">
              </a>
              <br>
              A spatial-enhanced vision-language-action model trained on 1.1 Million real robot episodes, purely
              huggingFace-based, concise code with efficient performance.
            </td>
          </tr>

          
          <tr onmouseout="gsslam_stop()" onmouseover="gsslam_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='gsslam_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202403-gsslam/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202403-gsslam/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function gsslam_start() {
                  document.getElementById('gsslam_shape').style.opacity = "1";
                }
                function gsslam_stop() {
                  document.getElementById('gsslam_shape').style.opacity = "0";
                }
                gsslam_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://gsslams.github.io/">
                <papertitle>
                  <img src="media/logo_oral.jpg" width="15">GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting
                </papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=IuGoHCkAAAAJ">Chi Yan*</a>,
              <strong>Delin Qu*</strong>,
              <a href="https://www.danxurgb.net">Dan Xu</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?user=cw3EaAYAAAAJ">Zhigang Wang</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024,
              <strong>(<span style="color:#c20000;">Spotlight</span>, top 2.6%)</strong>
              <br>
              <a href="https://arxiv.org/abs/2311.11700">paper</a> |
              <a href="https://gs-slam.github.io/">project page</a> |
              <a href="https://gs-slam.github.io/">video</a> |
              <a href="https://github.com/yanchi-3dv/diff-gaussian-rasterization-for-gsslam">code</a>
              <br>
              The first to utilize 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM)
              system.
            </td>
          </tr>

          <tr onmouseout="enslam_stop()" onmouseover="enslam_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='enslam_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202403-enslam/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202403-enslam/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function enslam_start() {
                  document.getElementById('enslam_shape').style.opacity = "1";
                }
                function enslam_stop() {
                  document.getElementById('enslam_shape').style.opacity = "0";
                }
                enslam_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://delinqu.github.io/EN-SLAM/">
                <papertitle>
                  <img src="media/logo_oral.jpg" width="15">Implicit Event-RGBD Neural SLAM
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://scholar.google.com/citations?user=IuGoHCkAAAAJ">Chi Yan*</a>,
              <a href="https://scholar.google.com/citations?user=Y8LVRYIAAAAJ&hl=en">Yin Jie</a>,
              <a href="https://github.com/Tavish9">Qizhi Chen</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?user=cw3EaAYAAAAJ">Zhigang Wang</a>,
              <a href="https://www.danxurgb.net">Dan Xu</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024,
              <strong>(<span style="color:#c20000;">Spotlight</span>, top 2.6%)</strong>
              <br>
              <a href="https://arxiv.org/abs/2311.11013">paper</a> |
              <a href="https://delinqu.github.io/EN-SLAM">project page</a> |
              <a href="https://delinqu.github.io/EN-SLAM">video</a> |
              <a href="https://delinqu.github.io/EN-SLAM">code</a> |
              <a href="https://huggingface.co/datasets/delinqu/EN-SLAM-Dataset">dataset <img alt="Static Badge"
                  src="https://img.shields.io/badge/ü§ó Huggingface-enslam-green"></a>
              <br>
              The first event-RGBD implicit neural SLAM that leverages event stream and RGBD to overcome challenges in
              motion blur and lighting variation scenes.
            </td>
          </tr>

          <tr onmouseout="mifag_stop()" onmouseover="mifag_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='mifag_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202410-mifag/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202410-mifag/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function mifag_start() {
                  document.getElementById('mifag_shape').style.opacity = "1";
                }
                function mifag_stop() {
                  document.getElementById('mifag_shape').style.opacity = "0";
                }
                mifag_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://goxq.github.io/mifag/">
                <papertitle>
                  <img src="media/logo_oral.jpg" width="15">Learning 2D Invariant Affordance Knowledge for 3D Affordance Grounding
                </papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=IuGoHCkAAAAJ">Xianqiang Gao</a>,
              <a href="https://github.com/zhangpingrui">Pinrui Zhang</a>,
              <strong>Delin Qu*</strong>,
              <a href="https://scholar.google.com/citations?user=cw3EaAYAAAAJ">Zhigang Wang</a>,
              <strong>Yan Ding</strong>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <em>the Association for the Advancement of Artificial Intelligence (<strong>(<span style="color:#c20000;">AAAI Oral</span>, top 4.6%)</strong>)</em>, 2025,
              <br>
              <a href="https://arxiv.org/abs/2408.13024">paper</a> |
              <a href="https://goxq.github.io/mifag">project page</a> |
              <a href="https://github.com/goxq/MIFAG-code">code</a>
              <br>
              A Multi-Image Guided Invariant Feature Aware 3D Affordance Grounding (MIFAG) framework.
            </td>
          </tr>

          <tr onmouseout="drsc_stop()" onmouseover="drsc_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='drsc_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202306-drsc/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202306-drsc/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function drsc_start() {
                  document.getElementById('drsc_shape').style.opacity = "1";
                }
                function drsc_stop() {
                  document.getElementById('drsc_shape').style.opacity = "0";
                }
                drsc_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://github.com/DelinQu/rspy">
                <papertitle>
                  <img src="media/logo_oral.jpg" width="15">
                  Fast Rolling Shutter Correction in the Wild
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://scholar.google.com/citations?user=0z2qluIAAAAJ">Bangyan Liao*</a>,
              <a href="https://dblp.org/pid/58/6739.html">Yifei Xue</a>,
              <a href="https://github.com/Kikihqq">Huiqing Zhang</a>,
              <a href="https://scholar.google.fr/citations?user=NIdLQnUAAAAJ&hl=fr">Omar Ait Aider</a>,
              <a href="https://yizhenlao.github.io">Yizhen Lao</a>.
              <br>
              IEEE Transactions on Pattern Analysis and Machine Intelligence <strong>(<span
                  style="color:#c20000;"><em>TPAMI</em></span>)</strong>, 2023
              <br>
              <a href="https://ieeexplore.ieee.org/document/10148802">paper</a> |
              <a href="https://github.com/DelinQu/rspy">project page</a> |
              <a href="https://github.com/DelinQu/rspy">video</a> |
              <a href="https://github.com/DelinQu/rspy">code</a> |
              <a href="https://github.com/DelinQu/Urban-Crossroads-Rolling-Shutter-Dataset">dataset <img
                  alt="Static Badge" src="https://img.shields.io/badge/ü§ó Huggingface-fast-green"></a>
              <br>
              A pixel-wise varying direct RS correction framework that handles locally varying distortion caused by
              various sources, such as camera motion, moving objects, and even highly varying depth scenes.
            </td>
          </tr>

          <tr onmouseout="freegs_stop()" onmouseover="freegs_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='freegs_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202502-freegaussian/freegaussian.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202502-freegaussian/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function freegs_start() {
                  document.getElementById('freegs_shape').style.opacity = "1";
                }
                function freegs_stop() {
                  document.getElementById('freegs_shape').style.opacity = "0";
                }
                freegs_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://freegaussian.github.io">
                <papertitle>
                  FreeGaussian: Guidance-free Controllable 3D Gaussian Splats with Flow Derivatives
                </papertitle>
              </a>
              <br>
              <a href="https://github.com/Tavish9">Qizhi Chen*</a>,
              <strong>Delin Qu*</strong>,
              <a href="https://github.com/HaomingSong">Haoming Song</a>,
              <a href="https://scholar.google.com.hk/citations?user=v-oVANQAAAAJ">Yiwen Tang</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <br>
              <em>the Association for the Advancement of Artificial Intelligence (<strong><span style="color:#c20000;">AAAI Oral</span></strong>, top 4.6%)</em>, 2026,
              <a href="https://arxiv.org/abs/2410.22070">paper</a> |
              <a href="https://freegaussian.github.io">project page</a> |
              <a href="https://freegaussian.github.io">video</a> |
              <a href="https://github.com/freegaussian/freegaussian.github.io">code</a>
              <br>
              An annotation guidance-free method, dubbed FreeGaussian, that mathematically derives dynamic Gaussian
              motion from optical flow and camera motion using novel dynamic Gaussian constraints.
            </td>
          </tr>

          <tr onmouseout="livescene_stop()" onmouseover="livescene_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='livescene_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202409-livescene/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202409-livescene/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function livescene_start() {
                  document.getElementById('livescene_shape').style.opacity = "1";
                }
                function livescene_stop() {
                  document.getElementById('livescene_shape').style.opacity = "0";
                }
                livescene_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://livescenes.github.io/">
                <papertitle>
                  LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://github.com/Tavish9">Qizhi Chen*</a>,
              <a href="https://github.com/zhangpingrui">Pingrui Zhang</a>,
              <a href="https://scholar.google.com/citations?user=oZSREOkAAAAJ">Xianqiang Gao</a>,
              <!-- <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>, -->
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <!-- <a href="https://scholar.google.com/citations?user=cw3EaAYAAAAJ">Zhigang Wang</a>, -->
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>Neurips</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2406.16038">paper</a> |
              <a href="https://livescenes.github.io">project page</a> |
              <a href="https://livescenes.github.io">video</a> |
              <a href="https://github.com/Tavish9/livescene">code</a> |
              <a href="https://huggingface.co/datasets/IPEC-COMMUNITY/LiveScene">dataset <img alt="Static Badge"
                  src="https://img.shields.io/badge/ü§ó Huggingface-livescene-green"></a>
              <br>
              Embedding language feature to interactive scenes, grounding and manipulating interactable objects with
              language instructions.
            </td>
          </tr>

          <tr onmouseout="qrsc_stop()" onmouseover="qrsc_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='qrsc_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202406-qrst/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202406-qrst/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function qrsc_start() {
                  document.getElementById('qrsc_shape').style.opacity = "1";
                }
                function qrsc_stop() {
                  document.getElementById('qrsc_shape').style.opacity = "0";
                }
                qrsc_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://delinqu.github.io/QRSC">
                <papertitle>
                  Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://yizhenlao.github.io">Yizhen Lao</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.com/citations?user=cw3EaAYAAAAJ">Zhigang Wang</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <br>
              <em>Proceedings of the IEEE/CVF International Conference on Computer Vision(<strong>ICCV</strong>)</em>,
              2023
              <br>
              <a
                href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qu_Towards_Nonlinear-Motion-Aware_and_Occlusion-Robust_Rolling_Shutter_Correction_ICCV_2023_paper.pdf">paper</a>
              |
              <a href="https://delinqu.github.io/QRSC/">project page</a> |
              <a href="https://www.youtube.com/watch?v=Or-yvKHUrZ0">video</a> |
              <a href="https://github.com/DelinQu/qrsc?tab=readme-ov-file">code</a>
              <br>
              A geometry-based Quadratic Rolling Shutter (QRS) motion solver, which precisely estimates the high-order
              correction field of individual pixels.
            </td>
          </tr>

          <tr onmouseout="rsba_stop()" onmouseover="rsba_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='rsba_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202303-rsba/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202303-rsba/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function rsba_start() {
                  document.getElementById('rsba_shape').style.opacity = "1";
                }
                function rsba_stop() {
                  document.getElementById('rsba_shape').style.opacity = "0";
                }
                rsba_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://delinqu.github.io/NW-RSBA">
                <papertitle>
                  Revisiting Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution
                </papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=0z2qluIAAAAJ">Bangyan Liao*</a>,
              <strong>Delin Qu*</strong>,
              <a href="https://dblp.org/pid/58/6739.html">Yifei Xue</a>,
              <a href="https://github.com/Kikihqq">Huiqing Zhang</a>,
              <a href="https://yizhenlao.github.io">Yizhen Lao</a>.
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a
                href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Revisiting_Rolling_Shutter_Bundle_Adjustment_Toward_Accurate_and_Fast_Solution_CVPR_2023_paper.pdf">paper</a>
              |
              <a href="https://delinqu.github.io/NW-RSBA">project page</a> |
              <a href="https://www.youtube.com/watch?v=aCo60XUatss">video</a> |
              <a href="https://github.com/DelinQu/NW-RSBA">code</a>
              <br>
              An accurate and fast bundle adjustment solution that estimates the 6-DoF pose with an independent RS model
              of the camera and the geometry of the environment based on measurements from a rolling shutter camera.
            </td>
          </tr>
        </table>

        <!-- Mentored Students -->
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Mentored Students</heading>
          <tr>
            <td>
              I am fortunate to (co-)mentor some talented and highly motivated
              students. I
              have learnt from and gotten inspired by them:
              <ul>
                <li> <a href="https://janackermann.info/"><strong>Jan
                      Ackermann</strong></a> (Ongoing): MSc student at ETH
                  Zurich<br>
                  <ul>
                    <li> Semester thesis: Continual Learning of Gaussian Splatting
                      with
                      Local Optimization (CVPR'25 submission)
                    <li> &#8594; Master thesis at Stanford University, advised by <a
                        href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
                  </ul>
                  <br>
                <li> <a href="https://www.linkedin.com/in/goncayilmaz/"><strong>Gonca
                      Yilmaz</strong></a> (2024): MSc student at University of
                  Zurich<br>
                  <ul>
                    <li> Semester thesis: Open Vocabulary Segmentation from
                      Multi-Modal
                      Inputs (ICCVW'23)
                    <li> Master thesis: <a href="https://open-das.github.io/">OpenDAS:
                        Open-Vocabulary Domain Adaption for Segmentation</a>
                      (ICLR'25
                      submission)
                    <li> &#8594; Software engineer at <a href="https://about.google/">Google</a>
                  </ul>
                  <br>
            </td>
          </tr>
        </table> -->

        <!-- Invited Talks -->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;"> -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Invited Talks</heading>
          <br><br><br>
          <tr onmouseout="teleai_stop()" onmouseover="teleai_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='talk_teleai'>
                  <img src='talk/202503_teleai/teaser2.png' width="180" height="100">
                </div>
                <img src='talk/202503_teleai/teaser1.png' width="180" height="100">
              </div>
              </div>
              <script type="text/javascript">
                function teleai_start() {
                  document.getElementById('talk_teleai').style.opacity = "1";
                }
                function teleai_stop() {
                  document.getElementById('talk_teleai').style.opacity = "0";
                }
                teleai_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="talk/202503_teleai/spatialvla_slides.pdf">
                <papertitle>Exploring Spatial Representations for Visual-Language-Action Model</papertitle>
              </a>
              <br>
              <em><strong>Institute of Artificial Intelligence (TeleAI), China Telecom</strong></em>, hosted by <a
                href="https://baichenjia.github.io/">Chenjia Bai</a>, Mar 2025<br>
              <br>
              A spatial-enhanced vision-language-action model that is trained on 1.1 Million real robot episodes,
              toward the More Generalist Agents System.
              <a href="talk/202503_teleai/spatialvla_slides.pdf">slides</a>
            </td>
          </tr>
        </table>

        <!-- Selected Projects -->
        <table width="100%" align="center" border="0" cellpadding="20" cellspacing="0">
          <heading>Selected Projects</heading>
          <br><br><br>
          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='comet'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="project/comet/comet_gh.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://github.com/mli0603/openpi-comet">
                <papertitle>
                  üèÜ Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge
                </papertitle>
              </a>
              <br>
              Delin Qu*, Qizhi Chen*, Shangkun Sun*, Zhaoshuo Li‚Ä†, Yu-Wei Chao, Xiaohui Zeng, Xuan Li, Junjie Bai, Tsung-Yi Lin, Ming-Yu Liu‚Ä†,
              <a href="https://arxiv.org/abs/2512.10071">paper</a> |
              <a href="https://github.com/mli0603/openpi-comet">video</a> |
              <a href="https://github.com/mli0603/openpi-comet">code</a> |
              <a href="project/comet/arward.png">üèÜ Award</a>
              <p></p>
              Focus on systematically building our solution by studying the effects of training techniques and data. Show the scaling power in pre-training
              and post-training phases for competitive performance in Behavior 1K Challenge.
              <p></p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='fastumi'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="project/fast-umi/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://fastumi.com">
                <papertitle>
                  <img src="media/logo_oral.jpg" width="15"> FastUMI: A Scalable and Hardware-Independent Universal
                  Manipulation Interface with Dataset
                </papertitle>
              </a>
              <br>
              Zhaxizhuoma, Kehui Liu, et.al, <strong>Delin Qu</strong>, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li
              <!-- <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025 <strong>(<spanstyle="color:#c20000;">Oral</span>, top 1.8%)</strong>
              <br> -->
              <a href="https://arxiv.org/abs/2409.19499">paper</a> |
              <a href="https://fastumi.com/">project page</a> |
              <a href="https://fastumi.com/">video</a> |
              <a href="https://github.com/RealRobotSquad/FastUMI_Data">code</a>
              <p></p>
              A substantial redesign of the Universal Manipulation Interface system enabling rapid deployment and
              delivering robust performance in real-world data acquisition.
              <p></p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='embllm'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="project/ood/ood.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="http://scis.scichina.com/en/2024/124201.pdf">
                <papertitle>
                  Optics-driven drone
                </papertitle>
              </a>
              <br>
              Xuelong Li, Guan Huang, Zhigang Wang, <strong>Delin Qu</strong>, Bin Zhao
              <br>
              <em>Science China. Information Sciences, 67(2), 124201</em>, 2024
              <br>
              <a href="http://scis.scichina.com/en/2024/124201.pdf">paper</a> |
              <a href="http://scis.scichina.com/en/2024/124201.pdf">project page</a> |
              <a href="http://scis.scichina.com/en/2024/124201.pdf">video</a> |
              <a href="http://scis.scichina.com/en/2024/124201.pdf">code</a>
              <p></p>
              A remote charging technology for drones to enhance their autonomy and intelligence during mission
              execution
              <p></p>
            </td>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='embllm'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="project/embllm/embllm.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="http://scis.scichina.com/en/2024/124201.pdf">
                <papertitle>
                  Large Model Heterogeneous Intelligent Agent Systems
                </papertitle>
              </a>
              <br>
              Kehui Liu, Zixin Tang, et.al, <strong>Delin Qu</strong>, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li
              <br>
              <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2409.15146v2">paper</a> |
              <a href="https://github.com/mrkeee/coherent">project page</a> |
              <a href="https://www.youtube.com/watch?v=dV1J-VXdEJA">video</a> |
              <a href="https://github.com/mrkeee/coherent">code</a>
              <p></p>
              A novel LLM-based task planning framework for collaboration of heterogeneous multi-robot systems including
              quadrotors, robotic dogs, and robotic arms.
              <p></p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='any4lerobot'>
                  <!-- <video width="160" height="120" muted autoplay loop>
                    <source src="project/any4lerobot/video.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video> -->
                  <img src="project/any4lerobot/teaser.png" width="160" height="120">
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://github.com/Tavish9/any4lerobot">
                <papertitle>
                  Any4LeRobot: A tool collection for LeRobot
                </papertitle>
              </a>
              <br>
              <a href="https://github.com/Tavish9/any4lerobot">paper</a> |
              <a href="https://github.com/Tavish9/any4lerobot">project page</a> |
              <a href="https://github.com/Tavish9/any4lerobot">video</a> |
              <a href="https://github.com/Tavish9/any4lerobot">code</a>
              <p></p>
              A curated collection of utilities for LeRobot Projects, including data conversion scripts, preprocessing tools, training workflow helpers and etc.
              <p></p>
            </td>
          </tr>
    </tr>

  </table>

  <!-- Honors & Awards -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>Honors & Awards</heading>
    <tr>
      <td>
        <ul>
          <li><strong>Sep 2022 - Now</strong>: 
            National Natural Science Foundation of China (NSFC) grant in 2024,
            Second place in the Behavior 1K Challenge in 2025,
            Top Outstanding PhD Student Scholarship of Fudan University in
            2025, Tencent Scholarship in 2023, Fudan University Master's Excellence Scholarship in 2022,
            Outstanding Student Award in 2023, Fudan University's Outstanding Youth League Member in 2024.</li>
          <li><strong>Sep 2018 - Jun 2022</strong>: National Scholarship in 2021, National Scholarship in 2020,
            National Inspirational Scholarship in 2019, Finalist Prize of Mathematical Contest in Modeling, Second
            Prize of Asia-Pacific Mathematical Contest in Modeling, Second Prize in National Internet of Things
            Design Contest, Second Prize in Internet Competition of Hunan Province, Excellence Award in the Huawei
            AI Cloud Cup, Huawei College Scholarship, Huawei Smart Base Future Star, Excellent Graduation Thesis.
          </li>
        </ul>
      </td>
    </tr>
  </table>

  <!-- Academic Services -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>Academic Services</heading>
    <tr>
      <td>
        <ul>
          <li><strong>Conference Reviewer</strong>: TPAMI, CVPR, ICCV, ECCV, ICLR, ICML, and NeurIPS.</li>
          <li><strong>2023 Spring</strong>: COMP130135.04 Object Oriented Programming, Teaching Assistant.</li>
          </li>
        </ul>
      </td>
    </tr>
  </table>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <br>
        <p align="right">
          <font size="2">
            template adapted from <a href="https://jonbarron.info/">
              <font size="2">this awesome website</font>
            </a>
          </font>
        </p>
      </td>
    </tr>
  </table>
  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script>
  <script type="text/javascript">
    try {
      var pageTracker = _gat._getTracker("UA-116734954-1");
      pageTracker._trackPageview();
    } catch (err) { }
  </script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-116734954-1');
  </script>
  </td>
  </tr>
  </table>
</body>

</html>